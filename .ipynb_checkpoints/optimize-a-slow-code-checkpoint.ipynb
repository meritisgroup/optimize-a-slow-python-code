{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimiser un code python\n",
    "\n",
    "La plus part du temps vous n'aurez jamais besoin d'optimiser un code python, tellement les problemes viendront de la base de donnée, des API, des librairies etc... \n",
    "\n",
    "Et même quand vous optimiserez du code, il s'agira plus d'astuce fonctionnel, de mise en cache bien placée, d'utilisation de l'outil adapté, de solution de parallélisation simple que de connaissance profonde du langage. \n",
    "\n",
    "D'ailleurs pourquoi Python est aussi lent ? Souvent on dit que Python est lent et que si tu veux utiliesr un truc rapide code directement en C etc... mais en vrai pourquoi Python est plus lent qu'un autre ? \n",
    "\n",
    "On verra ça plus tard.\n",
    "\n",
    "Surtout avant d'optimiser un code il faut le faire fonctionner correctement \"premature optimization is the root of all evil\". \n",
    "\n",
    "La méthode qui marche pour écrire un code performant en Python est déjà dans la [doc officielle] (https://wiki.python.org/moin/PythonSpeed/PerformanceTips) : \n",
    "\n",
    "1. Get it right.\n",
    "2. Test it's right.\n",
    "3. Profile if slow.\n",
    "4. Optimise.\n",
    "5. Repeat from 2.\n",
    "\n",
    "### Les profiler \n",
    "\n",
    "Les profiler, ceux inclus de base : [profile, et cProfile](https://docs.python.org/2/library/profile.html) dans le TP on va utiliser [pyinstrument](https://github.com/joerick/pyinstrument). \n",
    "\n",
    "Il y a aussi [line_profiler](https://github.com/rkern/line_profiler) et [pyflame](https://github.com/uber-archive/pyflame) mais ils ne sont plus maintenu. \n",
    "\n",
    "Profiler c'est une étape éssentiel de l'optimisation car sinon on risque fort d'optimiser un truc inutile. \n",
    "\n",
    "Une fois que vous aurez profilez vous verrez bien que le probleme vient de la DB, probablement des JOIN stupides, mais mettons que pour une fois ce ne soit pas le cas, alors, il faut alors optimiser le code ! \n",
    "\n",
    "\n",
    "### Les optimisations qu'on a vu jusqu'à maintenant\n",
    "\n",
    "#### Les slots\n",
    "Python permet de définir des slots, lorsqu'un des slots sont définis alors Python va allouer une quantité statique de mémoire pour stocker  des attributs au lieu de stocker les attributs dans un dict, l'accés au atributs sera plus rapide et les attributs seront stockées dans un array au lieu d'être stocké dans un dict ce qui est plus économe en mémoire, moins de mémoire consommé dans le programme c'est un GC qui va se lancer moins fréquemment et donc de la perf de gagnée. \n",
    "\n",
    "\n",
    "```python\n",
    "class MyClass(object):\n",
    "    __slots__ = ['name', 'identifier']\n",
    "    def __init__(self, name, identifier):\n",
    "        self.name = name\n",
    "        self.identifier = identifier\n",
    "        self.set_up()\n",
    "```\n",
    "#### Les générateurs\n",
    "Les générateurs aussi apportent un gain de performance en permettant déconomiser la mémoire. Dans l'exemple plus bas les listes n'existent jamais toutes les 3 en même temps en mémoire mais son créée, renvoyée, traitée, colléctée (probablement) l'une aprés l'autre. \n",
    "\n",
    "```python\n",
    "def huge_lists():\n",
    "    yield [0] * 800000000\n",
    "    yield [1] * 800000000\n",
    "    yield [2] * 800000000\n",
    "\n",
    "total_len = 0\n",
    "for l in huge_lists():\n",
    "    total_len += len(l)\n",
    "```\n",
    "#### Le cache\n",
    "Une technique d'optimisation bien commune est l'utilisation de cache pour éviter de dépenser des ressources pour obtenir un résultat qu'on a déjà obtenu précédemment. La module [functools](https://docs.python.org/fr/3.8/library/functools.html) de la librairie standard fournit `lru_cache` et `cached_property`. [Django](https://docs.djangoproject.com/fr/2.2/topics/cache/) fournit une fonctionnalité de cache, il existe une exension [Flask](https://flask-caching.readthedocs.io/en/latest/) qui permet entre autre de s'appuyer sur Redis pour le cache, il existe aussi plusieurs client en Python pour [memcached](https://memcached.org/). \n",
    "\n",
    "\n",
    "### Les optimisations à connaitre\n",
    "\n",
    "- utiliser la bonne collection \n",
    "- executer des taches parallelement \n",
    "- éviter les memory leaks\n",
    "- utiliser d'autres implémentation que CPython ; cython, numba, etc... \n",
    "- quand faire du calcul parallelle devient dur : utiliser spark ou autre\n",
    "- utiliser numpy si on manipule de grande listes\n",
    "- utiliser Gunicorn ou autre avec Flask\n",
    "- utiliser des jobs queue pour éxécuter des jobs de maniere asynchrone\n",
    "\n",
    "\n",
    "### Les optimisations dont on s'en fout\n",
    "\n",
    "[ici](https://wiki.python.org/moin/PythonSpeed/PerformanceTips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
